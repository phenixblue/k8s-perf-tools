No Baseline flag is: 1
Node: cluster1-control-plane
Node: cluster1-worker
Node: cluster1-worker2
Node: cluster1-worker3
Pod: netperf-host-924s4  in  default 
Pod: netperf-host-d4gkl  in  default 
Pod: netperf-host-flxvg  in  default 
Pod: netperf-host-v2xpf  in  default 
Pod: netperf-pod-4j6ts  in  default 
Pod: netperf-pod-77c9846498-7w5x9  in  default 
Pod: netperf-pod-77c9846498-qktwg  in  default 
Pod: netperf-pod-7kpcq  in  default 
Pod: netperf-pod-f2q9t  in  default 
Pod: netperf-pod-x4ls4  in  default 
service: netperf-server  in  default IP=198.19.74.45 <none> app=netperf-pod 
cluster1-worker HASH(0x131827a68) cluster1-worker2 HASH(0x1318d7f78) cluster1-worker3 HASH(0x131920328) cluster1-control-plane HASH(0x131920ce8)
netperf-pod-f2q9t HASH(0x131930e28) netperf-pod-77c9846498-qktwg HASH(0x131911d88) netperf-pod-7kpcq HASH(0x1319308b8) netperf-pod-4j6ts HASH(0x1319308a0) netperf-pod-77c9846498-7w5x9 HASH(0x131907228) netperf-pod-x4ls4 HASH(0x131931368)
netperf-server HASH(0x131930e40)
running command: kubectl exec -it netperf-pod-77c9846498-qktwg -- iperf -c 198.18.3.2 -i 1 -t 2 
------------------------------------------------------------
Client connecting to 198.18.3.2, TCP port 5001
TCP window size:  340 KByte (default)
------------------------------------------------------------
[  3] local 198.18.3.3 port 51956 connected with 198.18.3.2 port 5001
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0- 1.0 sec  8.36 GBytes  71.9 Gbits/sec
[  3]  1.0- 2.0 sec  8.47 GBytes  72.8 Gbits/sec
[  3]  0.0- 2.0 sec  16.8 GBytes  72.3 Gbits/sec
running command: kubectl exec -it netperf-pod-77c9846498-qktwg -- netperf -H 198.18.3.2 -l 2 -P 1 -t TCP_RR -- -r 32,1024 -o P50_LATENCY,P90_LATENCY,P99_LATENCY,THROUGHPUT,THROUGHPUT_UNITS 
MIGRATED TCP REQUEST/RESPONSE TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 198.18.3.2 (198.18.) port 0 AF_INET : first burst 0
50th Percentile Latency Microseconds,90th Percentile Latency Microseconds,99th Percentile Latency Microseconds,Throughput,Throughput Units
53,67,94,19072.83,Trans/s
running command: kubectl exec -it netperf-pod-77c9846498-qktwg -- netperf -H 198.18.3.2 -l 2 -P 1 -t TCP_CRR -- -r 32,1024 -o THROUGHPUT,THROUGHPUT_UNITS 
MIGRATED TCP Connect/Request/Response TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 198.18.3.2 (198.18.) port 0 AF_INET
Throughput,Throughput Units
10285.54,Trans/s
running command: kubectl exec -it netperf-pod-77c9846498-qktwg -- fortio load -qps 0 -c 1 -http1.0 -t 2s 198.18.3.2:8080 
Fortio 1.3.0 running at 0 queries per second, 5->5 procs, for 2s: 198.18.3.2:8080
22:45:00 I httprunner.go:82> Starting http test for 198.18.3.2:8080 with 1 threads at -1.0 qps
22:45:00 W http_client.go:142> Assuming http:// on missing scheme for '198.18.3.2:8080'
Starting at max qps with 1 thread(s) [gomax 5] for 2s
22:45:02 I periodic.go:533> T000 ended after 2.000101s : 5505 calls. qps=2752.361005769209
Ended after 2.000549s : 5505 calls. qps=2751.7
Aggregated Function Time : count 5505 avg 0.00036175586 +/- 0.0003786 min 0.000237 max 0.017336 sum 1.991466
# range, mid point, percentile, count
>= 0.000237 <= 0.001 , 0.0006185 , 99.55, 5480
> 0.001 <= 0.002 , 0.0015 , 99.80, 14
> 0.002 <= 0.003 , 0.0025 , 99.87, 4
> 0.003 <= 0.004 , 0.0035 , 99.91, 2
> 0.005 <= 0.006 , 0.0055 , 99.93, 1
> 0.008 <= 0.009 , 0.0085 , 99.95, 1
> 0.009 <= 0.01 , 0.0095 , 99.96, 1
> 0.016 <= 0.017336 , 0.016668 , 100.00, 2
# target 50% 0.000620171
# target 75% 0.000811826
# target 90% 0.000926819
# target 99% 0.000995815
# target 99.9% 0.0037475
Sockets used: 5506 (for perfect keepalive, would be 1)
Code 200 : 5505 (100.0 %)
Response Header Sizes : count 5505 avg 0 +/- 0 min 0 max 0 sum 0
Response Body/Total Sizes : count 5505 avg 75 +/- 0 min 75 max 75 sum 412875
All done 5505 calls (plus 1 warmup) 0.362 ms avg, 2751.7 qps
running command: kubectl exec -it netperf-pod-77c9846498-qktwg -- fortio load -qps 0 -c 1 -t 2s 198.18.3.2:8080 
Fortio 1.3.0 running at 0 queries per second, 5->5 procs, for 2s: 198.18.3.2:8080
22:45:02 I httprunner.go:82> Starting http test for 198.18.3.2:8080 with 1 threads at -1.0 qps
22:45:02 W http_client.go:142> Assuming http:// on missing scheme for '198.18.3.2:8080'
Starting at max qps with 1 thread(s) [gomax 5] for 2s
22:45:04 I periodic.go:533> T000 ended after 2.000074s : 10369 calls. qps=5184.308180597318
Ended after 2.000491s : 10369 calls. qps=5183.2
Aggregated Function Time : count 10369 avg 0.00019150776 +/- 9.567e-05 min 8.8e-05 max 0.005762 sum 1.985744
# range, mid point, percentile, count
>= 8.8e-05 <= 0.001 , 0.000544 , 99.86, 10354
> 0.001 <= 0.002 , 0.0015 , 99.95, 10
> 0.002 <= 0.003 , 0.0025 , 99.97, 2
> 0.003 <= 0.004 , 0.0035 , 99.99, 2
> 0.005 <= 0.005762 , 0.005381 , 100.00, 1
# target 50% 0.000544617
# target 75% 0.000772969
# target 90% 0.00090998
# target 99% 0.000992187
# target 99.9% 0.0014631
Sockets used: 1 (for perfect keepalive, would be 1)
Code 200 : 10369 (100.0 %)
Response Header Sizes : count 10369 avg 75 +/- 0 min 75 max 75 sum 777675
Response Body/Total Sizes : count 10369 avg 75 +/- 0 min 75 max 75 sum 777675
All done 10369 calls (plus 1 warmup) 0.192 ms avg, 5183.2 qps
running command: kubectl exec -it netperf-pod-77c9846498-qktwg -- fortio load -qps 0 -c 1 -grpc -ping -t 2s 198.18.3.2 
Fortio 1.3.0 running at 0 queries per second, 5->5 procs, for 2s: 198.18.3.2
22:45:04 I grpcrunner.go:152> Starting GRPC Ping test for 198.18.3.2 with 1*1 threads at -1.0 qps
Starting at max qps with 1 thread(s) [gomax 5] for 2s
22:45:06 I periodic.go:533> T000 ended after 2.000214s : 4316 calls. qps=2157.7691187042983
Ended after 2.000594s : 4316 calls. qps=2157.4
Aggregated Function Time : count 4316 avg 0.00046108086 +/- 0.0003353 min 0.000315 max 0.02027 sum 1.990025
# range, mid point, percentile, count
>= 0.000315 <= 0.001 , 0.0006575 , 99.37, 4289
> 0.001 <= 0.002 , 0.0015 , 99.77, 17
> 0.002 <= 0.003 , 0.0025 , 99.91, 6
> 0.003 <= 0.004 , 0.0035 , 99.95, 2
> 0.004 <= 0.005 , 0.0045 , 99.98, 1
> 0.02 <= 0.02027 , 0.020135 , 100.00, 1
# target 50% 0.000659577
# target 75% 0.000831945
# target 90% 0.000935366
# target 99% 0.000997418
# target 99.9% 0.00294733
Ping SERVING : 4316
All done 4316 calls (plus 1 warmup) 0.461 ms avg, 2157.4 qps
running command: kubectl exec -it netperf-pod-77c9846498-7w5x9 -- iperf -c 198.18.1.2 -i 1 -t 2 
------------------------------------------------------------
Client connecting to 198.18.1.2, TCP port 5001
TCP window size: 85.0 KByte (default)
------------------------------------------------------------
[  3] local 198.18.1.3 port 60296 connected with 198.18.1.2 port 5001
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0- 1.0 sec  8.25 GBytes  70.9 Gbits/sec
[  3]  1.0- 2.0 sec  8.14 GBytes  69.9 Gbits/sec
[  3]  0.0- 2.0 sec  16.4 GBytes  70.4 Gbits/sec
running command: kubectl exec -it netperf-pod-77c9846498-7w5x9 -- netperf -H 198.18.1.2 -l 2 -P 1 -t TCP_RR -- -r 32,1024 -o P50_LATENCY,P90_LATENCY,P99_LATENCY,THROUGHPUT,THROUGHPUT_UNITS 
MIGRATED TCP REQUEST/RESPONSE TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 198.18.1.2 (198.18.) port 0 AF_INET : first burst 0
50th Percentile Latency Microseconds,90th Percentile Latency Microseconds,99th Percentile Latency Microseconds,Throughput,Throughput Units
53,66,97,19085.07,Trans/s
running command: kubectl exec -it netperf-pod-77c9846498-7w5x9 -- netperf -H 198.18.1.2 -l 2 -P 1 -t TCP_CRR -- -r 32,1024 -o THROUGHPUT,THROUGHPUT_UNITS 
MIGRATED TCP Connect/Request/Response TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 198.18.1.2 (198.18.) port 0 AF_INET
Throughput,Throughput Units
10374.74,Trans/s
running command: kubectl exec -it netperf-pod-77c9846498-7w5x9 -- fortio load -qps 0 -c 1 -http1.0 -t 2s 198.18.1.2:8080 
Fortio 1.3.0 running at 0 queries per second, 5->5 procs, for 2s: 198.18.1.2:8080
22:45:17 I httprunner.go:82> Starting http test for 198.18.1.2:8080 with 1 threads at -1.0 qps
22:45:17 W http_client.go:142> Assuming http:// on missing scheme for '198.18.1.2:8080'
Starting at max qps with 1 thread(s) [gomax 5] for 2s
22:45:19 I periodic.go:533> T000 ended after 2.000373s : 5441 calls. qps=2719.9927213574665
Ended after 2.000824s : 5441 calls. qps=2719.4
Aggregated Function Time : count 5441 avg 0.00036613472 +/- 0.0003288 min 0.000223 max 0.017054 sum 1.992139
# range, mid point, percentile, count
>= 0.000223 <= 0.001 , 0.0006115 , 99.54, 5416
> 0.001 <= 0.002 , 0.0015 , 99.76, 12
> 0.002 <= 0.003 , 0.0025 , 99.89, 7
> 0.003 <= 0.004 , 0.0035 , 99.96, 4
> 0.014 <= 0.016 , 0.015 , 99.98, 1
> 0.016 <= 0.017054 , 0.016527 , 100.00, 1
# target 50% 0.000613222
# target 75% 0.000808405
# target 90% 0.000925514
# target 99% 0.00099578
# target 99.9% 0.00313975
Sockets used: 5442 (for perfect keepalive, would be 1)
Code 200 : 5441 (100.0 %)
Response Header Sizes : count 5441 avg 0 +/- 0 min 0 max 0 sum 0
Response Body/Total Sizes : count 5441 avg 75 +/- 0 min 75 max 75 sum 408075
All done 5441 calls (plus 1 warmup) 0.366 ms avg, 2719.4 qps
running command: kubectl exec -it netperf-pod-77c9846498-7w5x9 -- fortio load -qps 0 -c 1 -t 2s 198.18.1.2:8080 
Fortio 1.3.0 running at 0 queries per second, 5->5 procs, for 2s: 198.18.1.2:8080
22:45:19 I httprunner.go:82> Starting http test for 198.18.1.2:8080 with 1 threads at -1.0 qps
22:45:19 W http_client.go:142> Assuming http:// on missing scheme for '198.18.1.2:8080'
Starting at max qps with 1 thread(s) [gomax 5] for 2s
22:45:21 I periodic.go:533> T000 ended after 2.000187s : 10366 calls. qps=5182.515434806845
Ended after 2.000642s : 10366 calls. qps=5181.3
Aggregated Function Time : count 10366 avg 0.00019157544 +/- 7.799e-05 min 0.0001 max 0.004794 sum 1.985871
# range, mid point, percentile, count
>= 0.0001 <= 0.001 , 0.00055 , 99.90, 10356
> 0.001 <= 0.002 , 0.0015 , 99.98, 8
> 0.003 <= 0.004 , 0.0035 , 99.99, 1
> 0.004 <= 0.004794 , 0.004397 , 100.00, 1
# target 50% 0.000550391
# target 75% 0.00077563
# target 90% 0.000910774
# target 99% 0.00099186
# target 99.9% 0.000999968
Sockets used: 1 (for perfect keepalive, would be 1)
Code 200 : 10366 (100.0 %)
Response Header Sizes : count 10366 avg 75 +/- 0 min 75 max 75 sum 777450
Response Body/Total Sizes : count 10366 avg 75 +/- 0 min 75 max 75 sum 777450
All done 10366 calls (plus 1 warmup) 0.192 ms avg, 5181.3 qps
running command: kubectl exec -it netperf-pod-77c9846498-7w5x9 -- fortio load -qps 0 -c 1 -grpc -ping -t 2s 198.18.1.2 
Fortio 1.3.0 running at 0 queries per second, 5->5 procs, for 2s: 198.18.1.2
22:45:21 I grpcrunner.go:152> Starting GRPC Ping test for 198.18.1.2 with 1*1 threads at -1.0 qps
Starting at max qps with 1 thread(s) [gomax 5] for 2s
22:45:23 I periodic.go:533> T000 ended after 2.000221s : 4222 calls. qps=2110.76676027299
Ended after 2.000596s : 4222 calls. qps=2110.4
Aggregated Function Time : count 4222 avg 0.00047052037 +/- 0.0003988 min 0.000292 max 0.017157 sum 1.986537
# range, mid point, percentile, count
>= 0.000292 <= 0.001 , 0.000646 , 98.77, 4170
> 0.001 <= 0.002 , 0.0015 , 99.62, 36
> 0.002 <= 0.003 , 0.0025 , 99.81, 8
> 0.003 <= 0.004 , 0.0035 , 99.86, 2
> 0.004 <= 0.005 , 0.0045 , 99.88, 1
> 0.006 <= 0.007 , 0.0065 , 99.91, 1
> 0.007 <= 0.008 , 0.0075 , 99.93, 1
> 0.009 <= 0.01 , 0.0095 , 99.95, 1
> 0.01 <= 0.011 , 0.0105 , 99.98, 1
> 0.016 <= 0.017157 , 0.0165785 , 100.00, 1
# target 50% 0.000650331
# target 75% 0.000829581
# target 90% 0.000937131
# target 99% 0.00127167
# target 99.9% 0.006778
Ping SERVING : 4222
All done 4222 calls (plus 1 warmup) 0.471 ms avg, 2110.4 qps
running command: kubectl exec -it netperf-pod-7kpcq -- iperf -c 198.18.3.2 -i 1 -t 2 
------------------------------------------------------------
Client connecting to 198.18.3.2, TCP port 5001
TCP window size: 85.0 KByte (default)
------------------------------------------------------------
[  3] local 198.18.1.2 port 60164 connected with 198.18.3.2 port 5001
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0- 1.0 sec   707 KBytes  5.79 Mbits/sec
[  3]  1.0- 2.0 sec  0.00 Bytes  0.00 bits/sec
[  3]  0.0- 2.1 sec   707 KBytes  2.81 Mbits/sec
running command: kubectl exec -it netperf-pod-7kpcq -- netperf -H 198.18.3.2 -l 2 -P 1 -t TCP_RR -- -r 32,1024 -o P50_LATENCY,P90_LATENCY,P99_LATENCY,THROUGHPUT,THROUGHPUT_UNITS 
MIGRATED TCP REQUEST/RESPONSE TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 198.18.3.2 (198.18.) port 0 AF_INET : first burst 0
50th Percentile Latency Microseconds,90th Percentile Latency Microseconds,99th Percentile Latency Microseconds,Throughput,Throughput Units
64,79,109,15944.67,Trans/s
running command: kubectl exec -it netperf-pod-7kpcq -- netperf -H 198.18.3.2 -l 2 -P 1 -t TCP_CRR -- -r 32,1024 -o THROUGHPUT,THROUGHPUT_UNITS 
MIGRATED TCP Connect/Request/Response TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 198.18.3.2 (198.18.) port 0 AF_INET
Throughput,Throughput Units
7273.68,Trans/s
running command: kubectl exec -it netperf-pod-7kpcq -- fortio load -qps 0 -c 1 -http1.0 -t 2s 198.18.3.2:8080 
Fortio 1.3.0 running at 0 queries per second, 5->5 procs, for 2s: 198.18.3.2:8080
22:45:34 I httprunner.go:82> Starting http test for 198.18.3.2:8080 with 1 threads at -1.0 qps
22:45:34 W http_client.go:142> Assuming http:// on missing scheme for '198.18.3.2:8080'
Starting at max qps with 1 thread(s) [gomax 5] for 2s
22:45:36 I periodic.go:533> T000 ended after 2.000259s : 4960 calls. qps=2479.678881584835
Ended after 2.000693s : 4960 calls. qps=2479.1
Aggregated Function Time : count 4960 avg 0.00040180081 +/- 0.0002561 min 0.000253 max 0.015769 sum 1.992932
# range, mid point, percentile, count
>= 0.000253 <= 0.001 , 0.0006265 , 99.74, 4947
> 0.001 <= 0.002 , 0.0015 , 99.94, 10
> 0.002 <= 0.003 , 0.0025 , 99.96, 1
> 0.008 <= 0.009 , 0.0085 , 99.98, 1
> 0.014 <= 0.015769 , 0.0148845 , 100.00, 1
# target 50% 0.000627406
# target 75% 0.000814685
# target 90% 0.000927052
# target 99% 0.000994472
# target 99.9% 0.001804
Sockets used: 4961 (for perfect keepalive, would be 1)
Code 200 : 4960 (100.0 %)
Response Header Sizes : count 4960 avg 0 +/- 0 min 0 max 0 sum 0
Response Body/Total Sizes : count 4960 avg 75 +/- 0 min 75 max 75 sum 372000
All done 4960 calls (plus 1 warmup) 0.402 ms avg, 2479.1 qps
running command: kubectl exec -it netperf-pod-7kpcq -- fortio load -qps 0 -c 1 -t 2s 198.18.3.2:8080 
Fortio 1.3.0 running at 0 queries per second, 5->5 procs, for 2s: 198.18.3.2:8080
22:45:36 I httprunner.go:82> Starting http test for 198.18.3.2:8080 with 1 threads at -1.0 qps
22:45:36 W http_client.go:142> Assuming http:// on missing scheme for '198.18.3.2:8080'
Starting at max qps with 1 thread(s) [gomax 5] for 2s
22:45:38 I periodic.go:533> T000 ended after 2.000149s : 10055 calls. qps=5027.125479151803
Ended after 2.000498s : 10055 calls. qps=5026.2
Aggregated Function Time : count 10055 avg 0.00019769826 +/- 8.429e-05 min 9.2e-05 max 0.004794 sum 1.987856
# range, mid point, percentile, count
>= 9.2e-05 <= 0.001 , 0.000546 , 99.91, 10046
> 0.001 <= 0.002 , 0.0015 , 99.95, 4
> 0.002 <= 0.003 , 0.0025 , 99.97, 2
> 0.003 <= 0.004 , 0.0035 , 99.99, 2
> 0.004 <= 0.004794 , 0.004397 , 100.00, 1
# target 50% 0.000546362
# target 75% 0.000773588
# target 90% 0.000909923
# target 99% 0.000991724
# target 99.9% 0.000999905
Sockets used: 1 (for perfect keepalive, would be 1)
Code 200 : 10055 (100.0 %)
Response Header Sizes : count 10055 avg 75 +/- 0 min 75 max 75 sum 754125
Response Body/Total Sizes : count 10055 avg 75 +/- 0 min 75 max 75 sum 754125
All done 10055 calls (plus 1 warmup) 0.198 ms avg, 5026.2 qps
running command: kubectl exec -it netperf-pod-7kpcq -- fortio load -qps 0 -c 1 -grpc -ping -t 2s 198.18.3.2 
Fortio 1.3.0 running at 0 queries per second, 5->5 procs, for 2s: 198.18.3.2
22:45:38 I grpcrunner.go:152> Starting GRPC Ping test for 198.18.3.2 with 1*1 threads at -1.0 qps
Starting at max qps with 1 thread(s) [gomax 5] for 2s
22:45:40 I periodic.go:533> T000 ended after 2.000426s : 4344 calls. qps=2171.537462520483
Ended after 2.000891s : 4344 calls. qps=2171
Aggregated Function Time : count 4344 avg 0.00045819728 +/- 0.0002949 min 0.000288 max 0.017933 sum 1.990409
# range, mid point, percentile, count
>= 0.000288 <= 0.001 , 0.000644 , 99.38, 4317
> 0.001 <= 0.002 , 0.0015 , 99.86, 21
> 0.002 <= 0.003 , 0.0025 , 99.95, 4
> 0.004 <= 0.005 , 0.0045 , 99.98, 1
> 0.016 <= 0.017933 , 0.0169665 , 100.00, 1
# target 50% 0.000646145
# target 75% 0.000825299
# target 90% 0.000932792
# target 99% 0.000997288
# target 99.9% 0.002414
Ping SERVING : 4344
All done 4344 calls (plus 1 warmup) 0.458 ms avg, 2171.0 qps
running command: kubectl exec -it netperf-pod-f2q9t -- iperf -c 198.18.0.5 -i 1 -t 2 
------------------------------------------------------------
Client connecting to 198.18.0.5, TCP port 5001
TCP window size: 85.0 KByte (default)
------------------------------------------------------------
[  3] local 198.18.2.2 port 37278 connected with 198.18.0.5 port 5001
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0- 1.0 sec  5.33 GBytes  45.8 Gbits/sec
[  3]  1.0- 2.0 sec  5.32 GBytes  45.7 Gbits/sec
[  3]  0.0- 2.0 sec  10.7 GBytes  45.7 Gbits/sec
running command: kubectl exec -it netperf-pod-f2q9t -- netperf -H 198.18.0.5 -l 2 -P 1 -t TCP_RR -- -r 32,1024 -o P50_LATENCY,P90_LATENCY,P99_LATENCY,THROUGHPUT,THROUGHPUT_UNITS 
MIGRATED TCP REQUEST/RESPONSE TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 198.18.0.5 (198.18.) port 0 AF_INET : first burst 0
50th Percentile Latency Microseconds,90th Percentile Latency Microseconds,99th Percentile Latency Microseconds,Throughput,Throughput Units
64,79,108,15853.67,Trans/s
running command: kubectl exec -it netperf-pod-f2q9t -- netperf -H 198.18.0.5 -l 2 -P 1 -t TCP_CRR -- -r 32,1024 -o THROUGHPUT,THROUGHPUT_UNITS 
MIGRATED TCP Connect/Request/Response TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 198.18.0.5 (198.18.) port 0 AF_INET
Throughput,Throughput Units
7234.32,Trans/s
running command: kubectl exec -it netperf-pod-f2q9t -- fortio load -qps 0 -c 1 -http1.0 -t 2s 198.18.0.5:8080 
Fortio 1.3.0 running at 0 queries per second, 5->5 procs, for 2s: 198.18.0.5:8080
22:45:51 I httprunner.go:82> Starting http test for 198.18.0.5:8080 with 1 threads at -1.0 qps
22:45:51 W http_client.go:142> Assuming http:// on missing scheme for '198.18.0.5:8080'
Starting at max qps with 1 thread(s) [gomax 5] for 2s
22:45:53 I periodic.go:533> T000 ended after 2.000145s : 4734 calls. qps=2366.828404940642
Ended after 2.000545s : 4734 calls. qps=2366.4
Aggregated Function Time : count 4734 avg 0.00042084242 +/- 0.0003928 min 0.000244 max 0.015851 sum 1.992268
# range, mid point, percentile, count
>= 0.000244 <= 0.001 , 0.000622 , 99.45, 4708
> 0.001 <= 0.002 , 0.0015 , 99.77, 15
> 0.002 <= 0.003 , 0.0025 , 99.83, 3
> 0.003 <= 0.004 , 0.0035 , 99.87, 2
> 0.004 <= 0.005 , 0.0045 , 99.89, 1
> 0.006 <= 0.007 , 0.0065 , 99.92, 1
> 0.007 <= 0.008 , 0.0075 , 99.94, 1
> 0.01 <= 0.011 , 0.0105 , 99.96, 1
> 0.014 <= 0.015851 , 0.0149255 , 100.00, 2
# target 50% 0.000624008
# target 75% 0.000814092
# target 90% 0.000928142
# target 99% 0.000996573
# target 99.9% 0.006266
Sockets used: 4735 (for perfect keepalive, would be 1)
Code 200 : 4734 (100.0 %)
Response Header Sizes : count 4734 avg 0 +/- 0 min 0 max 0 sum 0
Response Body/Total Sizes : count 4734 avg 75 +/- 0 min 75 max 75 sum 355050
All done 4734 calls (plus 1 warmup) 0.421 ms avg, 2366.4 qps
running command: kubectl exec -it netperf-pod-f2q9t -- fortio load -qps 0 -c 1 -t 2s 198.18.0.5:8080 
Fortio 1.3.0 running at 0 queries per second, 5->5 procs, for 2s: 198.18.0.5:8080
22:45:53 I httprunner.go:82> Starting http test for 198.18.0.5:8080 with 1 threads at -1.0 qps
22:45:53 W http_client.go:142> Assuming http:// on missing scheme for '198.18.0.5:8080'
Starting at max qps with 1 thread(s) [gomax 5] for 2s
22:45:55 I periodic.go:533> T000 ended after 2.000131s : 10046 calls. qps=5022.671015048514
Ended after 2.000569s : 10046 calls. qps=5021.6
Aggregated Function Time : count 10046 avg 0.0001976964 +/- 8.318e-05 min 0.000102 max 0.00498 sum 1.986058
# range, mid point, percentile, count
>= 0.000102 <= 0.001 , 0.000551 , 99.93, 10039
> 0.001 <= 0.002 , 0.0015 , 99.97, 4
> 0.002 <= 0.003 , 0.0025 , 99.98, 1
> 0.004 <= 0.00498 , 0.00449 , 100.00, 2
# target 50% 0.000551268
# target 75% 0.000775947
# target 90% 0.000910755
# target 99% 0.000991639
# target 99.9% 0.000999728
Sockets used: 1 (for perfect keepalive, would be 1)
Code 200 : 10046 (100.0 %)
Response Header Sizes : count 10046 avg 75 +/- 0 min 75 max 75 sum 753450
Response Body/Total Sizes : count 10046 avg 75 +/- 0 min 75 max 75 sum 753450
All done 10046 calls (plus 1 warmup) 0.198 ms avg, 5021.6 qps
running command: kubectl exec -it netperf-pod-f2q9t -- fortio load -qps 0 -c 1 -grpc -ping -t 2s 198.18.0.5 
Fortio 1.3.0 running at 0 queries per second, 5->5 procs, for 2s: 198.18.0.5
22:45:55 I grpcrunner.go:152> Starting GRPC Ping test for 198.18.0.5 with 1*1 threads at -1.0 qps
Starting at max qps with 1 thread(s) [gomax 5] for 2s
22:45:57 I periodic.go:533> T000 ended after 2.000347s : 4333 calls. qps=2166.1241774552113
Ended after 2.000742s : 4333 calls. qps=2165.7
Aggregated Function Time : count 4333 avg 0.00045943503 +/- 0.0003197 min 0.000312 max 0.017329 sum 1.990732
# range, mid point, percentile, count
>= 0.000312 <= 0.001 , 0.000656 , 99.42, 4308
> 0.001 <= 0.002 , 0.0015 , 99.79, 16
> 0.002 <= 0.003 , 0.0025 , 99.84, 2
> 0.003 <= 0.004 , 0.0035 , 99.86, 1
> 0.004 <= 0.005 , 0.0045 , 99.93, 3
> 0.005 <= 0.006 , 0.0055 , 99.95, 1
> 0.006 <= 0.007 , 0.0065 , 99.98, 1
> 0.016 <= 0.017329 , 0.0166645 , 100.00, 1
# target 50% 0.000657917
# target 75% 0.000830955
# target 90% 0.000934778
# target 99% 0.000997072
# target 99.9% 0.00455567
Ping SERVING : 4333
All done 4333 calls (plus 1 warmup) 0.459 ms avg, 2165.7 qps
running command: kubectl exec -it netperf-pod-77c9846498-qktwg -- iperf -c 198.18.0.5 -i 1 -t 2 
------------------------------------------------------------
Client connecting to 198.18.0.5, TCP port 5001
TCP window size: 85.0 KByte (default)
------------------------------------------------------------
[  3] local 198.18.3.3 port 44662 connected with 198.18.0.5 port 5001
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0- 1.0 sec   707 KBytes  5.79 Mbits/sec
[  3]  1.0- 2.0 sec  0.00 Bytes  0.00 bits/sec
[  3]  0.0- 2.1 sec   707 KBytes  2.82 Mbits/sec
running command: kubectl exec -it netperf-pod-77c9846498-qktwg -- netperf -H 198.18.0.5 -l 2 -P 1 -t TCP_RR -- -r 32,1024 -o P50_LATENCY,P90_LATENCY,P99_LATENCY,THROUGHPUT,THROUGHPUT_UNITS 
MIGRATED TCP REQUEST/RESPONSE TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 198.18.0.5 (198.18.) port 0 AF_INET : first burst 0
50th Percentile Latency Microseconds,90th Percentile Latency Microseconds,99th Percentile Latency Microseconds,Throughput,Throughput Units
64,79,109,16219.84,Trans/s
running command: kubectl exec -it netperf-pod-77c9846498-qktwg -- netperf -H 198.18.0.5 -l 2 -P 1 -t TCP_CRR -- -r 32,1024 -o THROUGHPUT,THROUGHPUT_UNITS 
MIGRATED TCP Connect/Request/Response TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 198.18.0.5 (198.18.) port 0 AF_INET
Throughput,Throughput Units
7036.41,Trans/s
running command: kubectl exec -it netperf-pod-77c9846498-qktwg -- fortio load -qps 0 -c 1 -http1.0 -t 2s 198.18.0.5:8080 
Fortio 1.3.0 running at 0 queries per second, 5->5 procs, for 2s: 198.18.0.5:8080
22:46:08 I httprunner.go:82> Starting http test for 198.18.0.5:8080 with 1 threads at -1.0 qps
22:46:08 W http_client.go:142> Assuming http:// on missing scheme for '198.18.0.5:8080'
Starting at max qps with 1 thread(s) [gomax 5] for 2s
22:46:10 I periodic.go:533> T000 ended after 2.000325s : 4830 calls. qps=2414.6076262607326
Ended after 2.000757s : 4830 calls. qps=2414.1
Aggregated Function Time : count 4830 avg 0.00041265424 +/- 0.0002493 min 0.000265 max 0.015825 sum 1.99312
# range, mid point, percentile, count
>= 0.000265 <= 0.001 , 0.0006325 , 99.57, 4809
> 0.001 <= 0.002 , 0.0015 , 99.88, 15
> 0.002 <= 0.003 , 0.0025 , 99.96, 4
> 0.004 <= 0.005 , 0.0045 , 99.98, 1
> 0.014 <= 0.015825 , 0.0149125 , 100.00, 1
# target 50% 0.000634029
# target 75% 0.000818619
# target 90% 0.000929374
# target 99% 0.000995827
# target 99.9% 0.0022925
Sockets used: 4831 (for perfect keepalive, would be 1)
Code 200 : 4830 (100.0 %)
Response Header Sizes : count 4830 avg 0 +/- 0 min 0 max 0 sum 0
Response Body/Total Sizes : count 4830 avg 75 +/- 0 min 75 max 75 sum 362250
All done 4830 calls (plus 1 warmup) 0.413 ms avg, 2414.1 qps
running command: kubectl exec -it netperf-pod-77c9846498-qktwg -- fortio load -qps 0 -c 1 -t 2s 198.18.0.5:8080 
Fortio 1.3.0 running at 0 queries per second, 5->5 procs, for 2s: 198.18.0.5:8080
22:46:10 I httprunner.go:82> Starting http test for 198.18.0.5:8080 with 1 threads at -1.0 qps
22:46:10 W http_client.go:142> Assuming http:// on missing scheme for '198.18.0.5:8080'
Starting at max qps with 1 thread(s) [gomax 5] for 2s
22:46:12 I periodic.go:533> T000 ended after 2.000158s : 10264 calls. qps=5131.5946040262825
Ended after 2.000621s : 10264 calls. qps=5130.4
Aggregated Function Time : count 10264 avg 0.00019363883 +/- 6.37e-05 min 9.8e-05 max 0.003045 sum 1.987509
# range, mid point, percentile, count
>= 9.8e-05 <= 0.001 , 0.000549 , 99.90, 10254
> 0.001 <= 0.002 , 0.0015 , 99.97, 7
> 0.002 <= 0.003 , 0.0025 , 99.99, 2
> 0.003 <= 0.003045 , 0.0030225 , 100.00, 1
# target 50% 0.000549396
# target 75% 0.000775138
# target 90% 0.000910583
# target 99% 0.00099185
# target 99.9% 0.000999977
Sockets used: 1 (for perfect keepalive, would be 1)
Code 200 : 10264 (100.0 %)
Response Header Sizes : count 10264 avg 75 +/- 0 min 75 max 75 sum 769800
Response Body/Total Sizes : count 10264 avg 75 +/- 0 min 75 max 75 sum 769800
All done 10264 calls (plus 1 warmup) 0.194 ms avg, 5130.4 qps
running command: kubectl exec -it netperf-pod-77c9846498-qktwg -- fortio load -qps 0 -c 1 -grpc -ping -t 2s 198.18.0.5 
Fortio 1.3.0 running at 0 queries per second, 5->5 procs, for 2s: 198.18.0.5
22:46:12 I grpcrunner.go:152> Starting GRPC Ping test for 198.18.0.5 with 1*1 threads at -1.0 qps
Starting at max qps with 1 thread(s) [gomax 5] for 2s
22:46:14 I periodic.go:533> T000 ended after 2.000482s : 4353 calls. qps=2175.9755898828384
Ended after 2.000873s : 4353 calls. qps=2175.6
Aggregated Function Time : count 4353 avg 0.000457255 +/- 0.000313 min 0.000306 max 0.019205 sum 1.990431
# range, mid point, percentile, count
>= 0.000306 <= 0.001 , 0.000653 , 99.33, 4324
> 0.001 <= 0.002 , 0.0015 , 99.79, 20
> 0.002 <= 0.003 , 0.0025 , 99.93, 6
> 0.003 <= 0.004 , 0.0035 , 99.98, 2
> 0.018 <= 0.019205 , 0.0186025 , 100.00, 1
# target 50% 0.000655248
# target 75% 0.000829952
# target 90% 0.000934774
# target 99% 0.000997667
# target 99.9% 0.0027745
Ping SERVING : 4353
All done 4353 calls (plus 1 warmup) 0.457 ms avg, 2175.6 qps
running command: kubectl exec -it netperf-pod-7kpcq -- iperf -c 198.19.74.45 -i 1 -t 2 
------------------------------------------------------------
Client connecting to 198.19.74.45, TCP port 5001
TCP window size:  340 KByte (default)
------------------------------------------------------------
[  3] local 198.18.1.2 port 36994 connected with 198.19.74.45 port 5001
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0- 1.0 sec   707 KBytes  5.79 Mbits/sec
[  3]  1.0- 2.0 sec  0.00 Bytes  0.00 bits/sec
[  3]  0.0- 2.1 sec   707 KBytes  2.82 Mbits/sec
running command: kubectl exec -it netperf-pod-7kpcq -- fortio load -qps 0 -c 1 -http1.0 -t 2s 198.19.74.45:8080 
Fortio 1.3.0 running at 0 queries per second, 5->5 procs, for 2s: 198.19.74.45:8080
22:46:17 I httprunner.go:82> Starting http test for 198.19.74.45:8080 with 1 threads at -1.0 qps
22:46:17 W http_client.go:142> Assuming http:// on missing scheme for '198.19.74.45:8080'
Starting at max qps with 1 thread(s) [gomax 5] for 2s
22:46:19 I periodic.go:533> T000 ended after 2.000535s : 3589 calls. qps=1794.0200996233507
Ended after 2.00097s : 3589 calls. qps=1793.6
Aggregated Function Time : count 3589 avg 0.00055571413 +/- 0.000486 min 0.000289 max 0.016899 sum 1.994458
# range, mid point, percentile, count
>= 0.000289 <= 0.001 , 0.0006445 , 99.47, 3570
> 0.001 <= 0.002 , 0.0015 , 99.78, 11
> 0.003 <= 0.004 , 0.0035 , 99.83, 2
> 0.005 <= 0.006 , 0.0055 , 99.89, 2
> 0.009 <= 0.01 , 0.0095 , 99.92, 1
> 0.012 <= 0.014 , 0.013 , 99.94, 1
> 0.014 <= 0.016 , 0.015 , 99.97, 1
> 0.016 <= 0.016899 , 0.0164495 , 100.00, 1
# target 50% 0.000646293
# target 75% 0.000825039
# target 90% 0.000932287
# target 99% 0.000996635
# target 99.9% 0.009411
Sockets used: 3590 (for perfect keepalive, would be 1)
Code 200 : 3589 (100.0 %)
Response Header Sizes : count 3589 avg 0 +/- 0 min 0 max 0 sum 0
Response Body/Total Sizes : count 3589 avg 75 +/- 0 min 75 max 75 sum 269175
All done 3589 calls (plus 1 warmup) 0.556 ms avg, 1793.6 qps
running command: kubectl exec -it netperf-pod-7kpcq -- fortio load -qps 0 -c 1 -t 2s 198.19.74.45:8080 
Fortio 1.3.0 running at 0 queries per second, 5->5 procs, for 2s: 198.19.74.45:8080
22:46:19 I httprunner.go:82> Starting http test for 198.19.74.45:8080 with 1 threads at -1.0 qps
22:46:19 W http_client.go:142> Assuming http:// on missing scheme for '198.19.74.45:8080'
Starting at max qps with 1 thread(s) [gomax 5] for 2s
22:46:21 I periodic.go:533> T000 ended after 2.000082s : 10167 calls. qps=5083.291585045014
Ended after 2.00045s : 10167 calls. qps=5082.4
Aggregated Function Time : count 10167 avg 0.00019549838 +/- 0.0001638 min 9.6e-05 max 0.015389 sum 1.987632
# range, mid point, percentile, count
>= 9.6e-05 <= 0.001 , 0.000548 , 99.88, 10155
> 0.001 <= 0.002 , 0.0015 , 99.97, 9
> 0.002 <= 0.003 , 0.0025 , 99.99, 2
> 0.014 <= 0.015389 , 0.0146945 , 100.00, 1
# target 50% 0.00054849
# target 75% 0.000774779
# target 90% 0.000910553
# target 99% 0.000992017
# target 99.9% 0.00120367
Sockets used: 1 (for perfect keepalive, would be 1)
Code 200 : 10167 (100.0 %)
Response Header Sizes : count 10167 avg 75 +/- 0 min 75 max 75 sum 762525
Response Body/Total Sizes : count 10167 avg 75 +/- 0 min 75 max 75 sum 762525
All done 10167 calls (plus 1 warmup) 0.195 ms avg, 5082.4 qps
running command: kubectl exec -it netperf-pod-7kpcq -- fortio load -qps 0 -c 1 -grpc -ping -t 2s 198.19.74.45 
Fortio 1.3.0 running at 0 queries per second, 5->5 procs, for 2s: 198.19.74.45
22:46:21 I grpcrunner.go:152> Starting GRPC Ping test for 198.19.74.45 with 1*1 threads at -1.0 qps
Starting at max qps with 1 thread(s) [gomax 5] for 2s
22:46:23 I periodic.go:533> T000 ended after 2.000372s : 4500 calls. qps=2249.581577826524
Ended after 2.000757s : 4500 calls. qps=2249.1
Aggregated Function Time : count 4500 avg 0.00044224333 +/- 0.0002943 min 0.000288 max 0.016446 sum 1.990095
# range, mid point, percentile, count
>= 0.000288 <= 0.001 , 0.000644 , 99.00, 4455
> 0.001 <= 0.002 , 0.0015 , 99.71, 32
> 0.002 <= 0.003 , 0.0025 , 99.84, 6
> 0.003 <= 0.004 , 0.0035 , 99.93, 4
> 0.004 <= 0.005 , 0.0045 , 99.98, 2
> 0.016 <= 0.016446 , 0.016223 , 100.00, 1
# target 50% 0.000647517
# target 75% 0.000827355
# target 90% 0.000935258
# target 99% 0.001
# target 99.9% 0.003625
Ping SERVING : 4500
All done 4500 calls (plus 1 warmup) 0.442 ms avg, 2249.1 qps
running command: kubectl exec -it netperf-pod-f2q9t -- iperf -c 198.19.74.45 -i 1 -t 2 
------------------------------------------------------------
Client connecting to 198.19.74.45, TCP port 5001
TCP window size: 85.0 KByte (default)
------------------------------------------------------------
[  3] local 198.18.2.2 port 52102 connected with 198.19.74.45 port 5001
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0- 1.0 sec   707 KBytes  5.79 Mbits/sec
[  3]  1.0- 2.0 sec  0.00 Bytes  0.00 bits/sec
[  3]  0.0- 2.1 sec   707 KBytes  2.82 Mbits/sec
running command: kubectl exec -it netperf-pod-f2q9t -- fortio load -qps 0 -c 1 -http1.0 -t 2s 198.19.74.45:8080 
Fortio 1.3.0 running at 0 queries per second, 5->5 procs, for 2s: 198.19.74.45:8080
22:46:26 I httprunner.go:82> Starting http test for 198.19.74.45:8080 with 1 threads at -1.0 qps
22:46:26 W http_client.go:142> Assuming http:// on missing scheme for '198.19.74.45:8080'
Starting at max qps with 1 thread(s) [gomax 5] for 2s
22:46:28 I periodic.go:533> T000 ended after 2.00051s : 3601 calls. qps=1800.0409895476655
Ended after 2.000944s : 3601 calls. qps=1799.7
Aggregated Function Time : count 3601 avg 0.00055385615 +/- 0.0003315 min 0.000286 max 0.01819 sum 1.994436
# range, mid point, percentile, count
>= 0.000286 <= 0.001 , 0.000643 , 99.31, 3576
> 0.001 <= 0.002 , 0.0015 , 99.92, 22
> 0.003 <= 0.004 , 0.0035 , 99.94, 1
> 0.005 <= 0.006 , 0.0055 , 99.97, 1
> 0.018 <= 0.01819 , 0.018095 , 100.00, 1
# target 50% 0.000645397
# target 75% 0.000825195
# target 90% 0.000933074
# target 99% 0.000997801
# target 99.9% 0.00197268
Sockets used: 3602 (for perfect keepalive, would be 1)
Code 200 : 3601 (100.0 %)
Response Header Sizes : count 3601 avg 0 +/- 0 min 0 max 0 sum 0
Response Body/Total Sizes : count 3601 avg 75 +/- 0 min 75 max 75 sum 270075
All done 3601 calls (plus 1 warmup) 0.554 ms avg, 1799.7 qps
running command: kubectl exec -it netperf-pod-f2q9t -- fortio load -qps 0 -c 1 -t 2s 198.19.74.45:8080 
Fortio 1.3.0 running at 0 queries per second, 5->5 procs, for 2s: 198.19.74.45:8080
22:46:28 I httprunner.go:82> Starting http test for 198.19.74.45:8080 with 1 threads at -1.0 qps
22:46:28 W http_client.go:142> Assuming http:// on missing scheme for '198.19.74.45:8080'
Starting at max qps with 1 thread(s) [gomax 5] for 2s
22:46:30 I periodic.go:533> T000 ended after 2.000239s : 10205 calls. qps=5101.890324106269
Ended after 2.000706s : 10205 calls. qps=5100.7
Aggregated Function Time : count 10205 avg 0.00019485772 +/- 7.11e-05 min 0.0001 max 0.003411 sum 1.988523
# range, mid point, percentile, count
>= 0.0001 <= 0.001 , 0.00055 , 99.91, 10196
> 0.001 <= 0.002 , 0.0015 , 99.96, 5
> 0.002 <= 0.003 , 0.0025 , 99.98, 2
> 0.003 <= 0.003411 , 0.0032055 , 100.00, 2
# target 50% 0.000550353
# target 75% 0.000775574
# target 90% 0.000910706
# target 99% 0.000991786
# target 99.9% 0.000999894
Sockets used: 1 (for perfect keepalive, would be 1)
Code 200 : 10205 (100.0 %)
Response Header Sizes : count 10205 avg 75 +/- 0 min 75 max 75 sum 765375
Response Body/Total Sizes : count 10205 avg 75 +/- 0 min 75 max 75 sum 765375
All done 10205 calls (plus 1 warmup) 0.195 ms avg, 5100.7 qps
running command: kubectl exec -it netperf-pod-f2q9t -- fortio load -qps 0 -c 1 -grpc -ping -t 2s 198.19.74.45 
Fortio 1.3.0 running at 0 queries per second, 5->5 procs, for 2s: 198.19.74.45
22:46:30 I grpcrunner.go:152> Starting GRPC Ping test for 198.19.74.45 with 1*1 threads at -1.0 qps
Starting at max qps with 1 thread(s) [gomax 5] for 2s
22:46:32 I periodic.go:533> T000 ended after 2.000206s : 4387 calls. qps=2193.274092768445
Ended after 2.000569s : 4387 calls. qps=2192.9
Aggregated Function Time : count 4387 avg 0.00045380397 +/- 0.0002787 min 0.000292 max 0.016153 sum 1.990838
# range, mid point, percentile, count
>= 0.000292 <= 0.001 , 0.000646 , 99.18, 4351
> 0.001 <= 0.002 , 0.0015 , 99.77, 26
> 0.002 <= 0.003 , 0.0025 , 99.89, 5
> 0.003 <= 0.004 , 0.0035 , 99.98, 4
> 0.016 <= 0.016153 , 0.0160765 , 100.00, 1
# target 50% 0.000648848
# target 75% 0.000827354
# target 90% 0.000934457
# target 99% 0.000998719
# target 99.9% 0.00315325
Ping SERVING : 4387
All done 4387 calls (plus 1 warmup) 0.454 ms avg, 2192.9 qps
running command: kubectl exec -it netperf-pod-77c9846498-qktwg -- iperf -c 198.19.74.45 -i 1 -t 2 
------------------------------------------------------------
Client connecting to 198.19.74.45, TCP port 5001
TCP window size:  340 KByte (default)
------------------------------------------------------------
[  3] local 198.18.3.3 port 58016 connected with 198.19.74.45 port 5001
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0- 1.0 sec   707 KBytes  5.79 Mbits/sec
[  3]  1.0- 2.0 sec  0.00 Bytes  0.00 bits/sec
[  3]  0.0- 2.1 sec   707 KBytes  2.81 Mbits/sec
running command: kubectl exec -it netperf-pod-77c9846498-qktwg -- fortio load -qps 0 -c 1 -http1.0 -t 2s 198.19.74.45:8080 
Fortio 1.3.0 running at 0 queries per second, 5->5 procs, for 2s: 198.19.74.45:8080
22:46:35 I httprunner.go:82> Starting http test for 198.19.74.45:8080 with 1 threads at -1.0 qps
22:46:35 W http_client.go:142> Assuming http:// on missing scheme for '198.19.74.45:8080'
Starting at max qps with 1 thread(s) [gomax 5] for 2s
22:46:37 I periodic.go:533> T000 ended after 2.000189s : 3644 calls. qps=1821.8278372693778
Ended after 2.000626s : 3644 calls. qps=1821.4
Aggregated Function Time : count 3644 avg 0.00054723793 +/- 0.0002772 min 0.000284 max 0.015495 sum 1.994135
# range, mid point, percentile, count
>= 0.000284 <= 0.001 , 0.000642 , 99.37, 3621
> 0.001 <= 0.002 , 0.0015 , 99.95, 21
> 0.002 <= 0.003 , 0.0025 , 99.97, 1
> 0.014 <= 0.015495 , 0.0147475 , 100.00, 1
# target 50% 0.000644176
# target 75% 0.000824362
# target 90% 0.000932474
# target 99% 0.000997342
# target 99.9% 0.00192171
Sockets used: 3645 (for perfect keepalive, would be 1)
Code 200 : 3644 (100.0 %)
Response Header Sizes : count 3644 avg 0 +/- 0 min 0 max 0 sum 0
Response Body/Total Sizes : count 3644 avg 75 +/- 0 min 75 max 75 sum 273300
All done 3644 calls (plus 1 warmup) 0.547 ms avg, 1821.4 qps
running command: kubectl exec -it netperf-pod-77c9846498-qktwg -- fortio load -qps 0 -c 1 -t 2s 198.19.74.45:8080 
Fortio 1.3.0 running at 0 queries per second, 5->5 procs, for 2s: 198.19.74.45:8080
22:46:37 I httprunner.go:82> Starting http test for 198.19.74.45:8080 with 1 threads at -1.0 qps
22:46:37 W http_client.go:142> Assuming http:// on missing scheme for '198.19.74.45:8080'
Starting at max qps with 1 thread(s) [gomax 5] for 2s
22:46:39 I periodic.go:533> T000 ended after 2.000259s : 10230 calls. qps=5114.337693268722
Ended after 2.000707s : 10230 calls. qps=5113.2
Aggregated Function Time : count 10230 avg 0.0001942955 +/- 0.0001296 min 8.8e-05 max 0.008052 sum 1.987643
# range, mid point, percentile, count
>= 8.8e-05 <= 0.001 , 0.000544 , 99.83, 10213
> 0.001 <= 0.002 , 0.0015 , 99.94, 11
> 0.002 <= 0.003 , 0.0025 , 99.96, 2
> 0.003 <= 0.004 , 0.0035 , 99.97, 1
> 0.004 <= 0.005 , 0.0045 , 99.98, 1
> 0.006 <= 0.007 , 0.0065 , 99.99, 1
> 0.008 <= 0.008052 , 0.008026 , 100.00, 1
# target 50% 0.000544714
# target 75% 0.000773116
# target 90% 0.000910157
# target 99% 0.000992382
# target 99.9% 0.00161545
Sockets used: 1 (for perfect keepalive, would be 1)
Code 200 : 10230 (100.0 %)
Response Header Sizes : count 10230 avg 75 +/- 0 min 75 max 75 sum 767250
Response Body/Total Sizes : count 10230 avg 75 +/- 0 min 75 max 75 sum 767250
All done 10230 calls (plus 1 warmup) 0.194 ms avg, 5113.2 qps
running command: kubectl exec -it netperf-pod-77c9846498-qktwg -- fortio load -qps 0 -c 1 -grpc -ping -t 2s 198.19.74.45 
Fortio 1.3.0 running at 0 queries per second, 5->5 procs, for 2s: 198.19.74.45
22:46:39 I grpcrunner.go:152> Starting GRPC Ping test for 198.19.74.45 with 1*1 threads at -1.0 qps
Starting at max qps with 1 thread(s) [gomax 5] for 2s
22:46:41 I periodic.go:533> T000 ended after 2.000127s : 4415 calls. qps=2207.3598326506267
Ended after 2.000481s : 4415 calls. qps=2207
Aggregated Function Time : count 4415 avg 0.00045080113 +/- 0.0002806 min 0.000317 max 0.016876 sum 1.990287
# range, mid point, percentile, count
>= 0.000317 <= 0.001 , 0.0006585 , 99.37, 4387
> 0.001 <= 0.002 , 0.0015 , 99.77, 18
> 0.002 <= 0.003 , 0.0025 , 99.91, 6
> 0.003 <= 0.004 , 0.0035 , 99.98, 3
> 0.016 <= 0.016876 , 0.016438 , 100.00, 1
# target 50% 0.000660602
# target 75% 0.000832481
# target 90% 0.000935609
# target 99% 0.000997485
# target 99.9% 0.00293083
Ping SERVING : 4415
All done 4415 calls (plus 1 warmup) 0.451 ms avg, 2207.0 qps
No Baseline flag is: 1
Node: meetup1-c8q39
Node: meetup1-c8q83
Node: meetup1-c8q8n
Pod: netperf-host-874pw  in  default 
Pod: netperf-host-g2cc2  in  default 
Pod: netperf-host-qv44k  in  default 
Pod: netperf-pod-77c9846498-cwqf4  in  default 
Pod: netperf-pod-77c9846498-pplxf  in  default 
Pod: netperf-pod-cxjzt  in  default 
Pod: netperf-pod-d4wq7  in  default 
Pod: netperf-pod-dvrd4  in  default 
service: netperf-server  in  default IP=10.245.186.73 <none> app=netperf-pod 
meetup1-c8q83 HASH(0x151015a50) meetup1-c8q8n HASH(0x151015ae0) meetup1-c8q39 HASH(0x1510724e8)
netperf-pod-77c9846498-cwqf4 HASH(0x151082ca0) netperf-pod-d4wq7 HASH(0x151071e58) netperf-pod-cxjzt HASH(0x1510268f0) netperf-pod-77c9846498-pplxf HASH(0x1510270b8) netperf-pod-dvrd4 HASH(0x151082cb8)
netperf-server HASH(0x151083240)
running command: kubectl exec -it netperf-pod-77c9846498-pplxf -- iperf -c 10.244.1.77 -i 1 -t 2 
------------------------------------------------------------
Client connecting to 10.244.1.77, TCP port 5001
TCP window size: 85.0 KByte (default)
------------------------------------------------------------
[  3] local 10.244.1.119 port 53916 connected with 10.244.1.77 port 5001
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0- 1.0 sec  2.64 GBytes  22.7 Gbits/sec
[  3]  1.0- 2.0 sec  2.76 GBytes  23.7 Gbits/sec
[  3]  0.0- 2.0 sec  5.41 GBytes  23.2 Gbits/sec
running command: kubectl exec -it netperf-pod-77c9846498-pplxf -- netperf -H 10.244.1.77 -l 2 -P 1 -t TCP_RR -- -r 32,1024 -o P50_LATENCY,P90_LATENCY,P99_LATENCY,THROUGHPUT,THROUGHPUT_UNITS 
MIGRATED TCP REQUEST/RESPONSE TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 10.244.1.77 (10.244.) port 0 AF_INET : first burst 0
50th Percentile Latency Microseconds,90th Percentile Latency Microseconds,99th Percentile Latency Microseconds,Throughput,Throughput Units
17,30,46,46240.48,Trans/s
running command: kubectl exec -it netperf-pod-77c9846498-pplxf -- netperf -H 10.244.1.77 -l 2 -P 1 -t TCP_CRR -- -r 32,1024 -o THROUGHPUT,THROUGHPUT_UNITS 
MIGRATED TCP Connect/Request/Response TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 10.244.1.77 (10.244.) port 0 AF_INET
Throughput,Throughput Units
11122.23,Trans/s
running command: kubectl exec -it netperf-pod-77c9846498-pplxf -- fortio load -qps 0 -c 1 -http1.0 -t 2s 10.244.1.77:8080 
Fortio 1.3.0 running at 0 queries per second, 4->4 procs, for 2s: 10.244.1.77:8080
18:43:53 I httprunner.go:82> Starting http test for 10.244.1.77:8080 with 1 threads at -1.0 qps
18:43:53 W http_client.go:142> Assuming http:// on missing scheme for '10.244.1.77:8080'
Starting at max qps with 1 thread(s) [gomax 4] for 2s
18:43:55 I periodic.go:533> T000 ended after 2.000180899s : 9396 calls. qps=4697.575106680388
Ended after 2.00022938s : 9396 calls. qps=4697.5
Aggregated Function Time : count 9396 avg 0.00021256648 +/- 0.0001087 min 0.000119533 max 0.004994406 sum 1.99727467
# range, mid point, percentile, count
>= 0.000119533 <= 0.001 , 0.000559767 , 99.87, 9384
> 0.001 <= 0.002 , 0.0015 , 99.95, 7
> 0.002 <= 0.003 , 0.0025 , 99.96, 1
> 0.003 <= 0.004 , 0.0035 , 99.98, 2
> 0.004 <= 0.00499441 , 0.0044972 , 100.00, 2
# target 50% 0.000560283
# target 75% 0.000780704
# target 90% 0.000912957
# target 99% 0.000992309
# target 99.9% 0.001372
Sockets used: 9397 (for perfect keepalive, would be 1)
Code 200 : 9396 (100.0 %)
Response Header Sizes : count 9396 avg 0 +/- 0 min 0 max 0 sum 0
Response Body/Total Sizes : count 9396 avg 75 +/- 0 min 75 max 75 sum 704700
All done 9396 calls (plus 1 warmup) 0.213 ms avg, 4697.5 qps
running command: kubectl exec -it netperf-pod-77c9846498-pplxf -- fortio load -qps 0 -c 1 -t 2s 10.244.1.77:8080 
Fortio 1.3.0 running at 0 queries per second, 4->4 procs, for 2s: 10.244.1.77:8080
18:43:56 I httprunner.go:82> Starting http test for 10.244.1.77:8080 with 1 threads at -1.0 qps
18:43:56 W http_client.go:142> Assuming http:// on missing scheme for '10.244.1.77:8080'
Starting at max qps with 1 thread(s) [gomax 4] for 2s
18:43:58 I periodic.go:533> T000 ended after 2.000049092s : 22824 calls. qps=11411.719887923633
Ended after 2.000110194s : 22824 calls. qps=11411
Aggregated Function Time : count 22824 avg 8.7409258e-05 +/- 0.0001618 min 3.5237e-05 max 0.012154871 sum 1.99502891
# range, mid point, percentile, count
>= 3.5237e-05 <= 0.001 , 0.000517619 , 99.84, 22788
> 0.001 <= 0.002 , 0.0015 , 99.92, 18
> 0.002 <= 0.003 , 0.0025 , 99.94, 4
> 0.003 <= 0.004 , 0.0035 , 99.96, 4
> 0.004 <= 0.005 , 0.0045 , 99.98, 5
> 0.006 <= 0.007 , 0.0065 , 99.98, 1
> 0.007 <= 0.008 , 0.0075 , 99.99, 2
> 0.008 <= 0.009 , 0.0085 , 100.00, 1
> 0.012 <= 0.0121549 , 0.0120774 , 100.00, 1
# target 50% 0.000518359
# target 75% 0.000759942
# target 90% 0.000904891
# target 99% 0.000991861
# target 99.9% 0.001732
Sockets used: 1 (for perfect keepalive, would be 1)
Code 200 : 22824 (100.0 %)
Response Header Sizes : count 22824 avg 75 +/- 0 min 75 max 75 sum 1711800
Response Body/Total Sizes : count 22824 avg 75 +/- 0 min 75 max 75 sum 1711800
All done 22824 calls (plus 1 warmup) 0.087 ms avg, 11411.4 qps
running command: kubectl exec -it netperf-pod-77c9846498-pplxf -- fortio load -qps 0 -c 1 -grpc -ping -t 2s 10.244.1.77 
Fortio 1.3.0 running at 0 queries per second, 4->4 procs, for 2s: 10.244.1.77
18:43:59 I grpcrunner.go:152> Starting GRPC Ping test for 10.244.1.77 with 1*1 threads at -1.0 qps
Starting at max qps with 1 thread(s) [gomax 4] for 2s
18:44:01 I periodic.go:533> T000 ended after 2.000045562s : 13177 calls. qps=6588.349910800683
Ended after 2.000090001s : 13177 calls. qps=6588.2
Aggregated Function Time : count 13177 avg 0.00015142899 +/- 0.0001046 min 8.1797e-05 max 0.004356029 sum 1.99537974
# range, mid point, percentile, count
>= 8.1797e-05 <= 0.001 , 0.000540899 , 99.81, 13152
> 0.001 <= 0.002 , 0.0015 , 99.95, 19
> 0.002 <= 0.003 , 0.0025 , 99.96, 1
> 0.003 <= 0.004 , 0.0035 , 99.98, 2
> 0.004 <= 0.00435603 , 0.00417801 , 100.00, 3
# target 50% 0.000541736
# target 75% 0.000771741
# target 90% 0.000909744
# target 99% 0.000992545
# target 99.9% 0.00162226
Ping SERVING : 13177
All done 13177 calls (plus 1 warmup) 0.151 ms avg, 6588.2 qps
running command: kubectl exec -it netperf-pod-77c9846498-cwqf4 -- iperf -c 10.244.0.33 -i 1 -t 2 
------------------------------------------------------------
Client connecting to 10.244.0.33, TCP port 5001
TCP window size: 85.0 KByte (default)
------------------------------------------------------------
[  3] local 10.244.0.56 port 35852 connected with 10.244.0.33 port 5001
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0- 1.0 sec   830 MBytes  6.96 Gbits/sec
[  3]  1.0- 2.0 sec  1.91 GBytes  16.4 Gbits/sec
[  3]  0.0- 2.0 sec  2.72 GBytes  11.7 Gbits/sec
running command: kubectl exec -it netperf-pod-77c9846498-cwqf4 -- netperf -H 10.244.0.33 -l 2 -P 1 -t TCP_RR -- -r 32,1024 -o P50_LATENCY,P90_LATENCY,P99_LATENCY,THROUGHPUT,THROUGHPUT_UNITS 
MIGRATED TCP REQUEST/RESPONSE TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 10.244.0.33 (10.244.) port 0 AF_INET : first burst 0
50th Percentile Latency Microseconds,90th Percentile Latency Microseconds,99th Percentile Latency Microseconds,Throughput,Throughput Units
37,47,106,24273.25,Trans/s
running command: kubectl exec -it netperf-pod-77c9846498-cwqf4 -- netperf -H 10.244.0.33 -l 2 -P 1 -t TCP_CRR -- -r 32,1024 -o THROUGHPUT,THROUGHPUT_UNITS 
MIGRATED TCP Connect/Request/Response TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 10.244.0.33 (10.244.) port 0 AF_INET
Throughput,Throughput Units
7229.82,Trans/s
running command: kubectl exec -it netperf-pod-77c9846498-cwqf4 -- fortio load -qps 0 -c 1 -http1.0 -t 2s 10.244.0.33:8080 
Fortio 1.3.0 running at 0 queries per second, 4->4 procs, for 2s: 10.244.0.33:8080
18:44:14 I httprunner.go:82> Starting http test for 10.244.0.33:8080 with 1 threads at -1.0 qps
18:44:14 W http_client.go:142> Assuming http:// on missing scheme for '10.244.0.33:8080'
Starting at max qps with 1 thread(s) [gomax 4] for 2s
18:44:16 I periodic.go:533> T000 ended after 2.000048046s : 4859 calls. qps=2429.441637523542
Ended after 2.00012186s : 4859 calls. qps=2429.4
Aggregated Function Time : count 4859 avg 0.00041112052 +/- 0.0004546 min 0.000164204 max 0.01595046 sum 1.9976346
# range, mid point, percentile, count
>= 0.000164204 <= 0.001 , 0.000582102 , 97.82, 4753
> 0.001 <= 0.002 , 0.0015 , 99.05, 60
> 0.002 <= 0.003 , 0.0025 , 99.53, 23
> 0.003 <= 0.004 , 0.0035 , 99.71, 9
> 0.004 <= 0.005 , 0.0045 , 99.86, 7
> 0.005 <= 0.006 , 0.0055 , 99.92, 3
> 0.007 <= 0.008 , 0.0075 , 99.94, 1
> 0.01 <= 0.011 , 0.0105 , 99.96, 1
> 0.011 <= 0.012 , 0.0115 , 99.98, 1
> 0.014 <= 0.0159505 , 0.0149752 , 100.00, 1
# target 50% 0.000591336
# target 75% 0.00080499
# target 90% 0.000933182
# target 99% 0.00195683
# target 99.9% 0.00571367
Sockets used: 4860 (for perfect keepalive, would be 1)
Code 200 : 4859 (100.0 %)
Response Header Sizes : count 4859 avg 0 +/- 0 min 0 max 0 sum 0
Response Body/Total Sizes : count 4859 avg 75 +/- 0 min 75 max 75 sum 364425
All done 4859 calls (plus 1 warmup) 0.411 ms avg, 2429.4 qps
running command: kubectl exec -it netperf-pod-77c9846498-cwqf4 -- fortio load -qps 0 -c 1 -t 2s 10.244.0.33:8080 
Fortio 1.3.0 running at 0 queries per second, 4->4 procs, for 2s: 10.244.0.33:8080
18:44:17 I httprunner.go:82> Starting http test for 10.244.0.33:8080 with 1 threads at -1.0 qps
18:44:17 W http_client.go:142> Assuming http:// on missing scheme for '10.244.0.33:8080'
Starting at max qps with 1 thread(s) [gomax 4] for 2s
18:44:19 I periodic.go:533> T000 ended after 2.000213438s : 14538 calls. qps=7268.224342366408
Ended after 2.000324636s : 14538 calls. qps=7267.8
Aggregated Function Time : count 14538 avg 0.00013725797 +/- 0.0002976 min 4.4304e-05 max 0.018509165 sum 1.99545644
# range, mid point, percentile, count
>= 4.4304e-05 <= 0.001 , 0.000522152 , 99.48, 14462
> 0.001 <= 0.002 , 0.0015 , 99.83, 52
> 0.002 <= 0.003 , 0.0025 , 99.92, 13
> 0.003 <= 0.004 , 0.0035 , 99.94, 2
> 0.004 <= 0.005 , 0.0045 , 99.96, 3
> 0.006 <= 0.007 , 0.0065 , 99.97, 2
> 0.012 <= 0.014 , 0.013 , 99.98, 1
> 0.014 <= 0.016 , 0.015 , 99.99, 2
> 0.018 <= 0.0185092 , 0.0182546 , 100.00, 1
# target 50% 0.00052463
# target 75% 0.000764826
# target 90% 0.000908944
# target 99% 0.000995415
# target 99.9% 0.00272785
Sockets used: 1 (for perfect keepalive, would be 1)
Code 200 : 14538 (100.0 %)
Response Header Sizes : count 14538 avg 75 +/- 0 min 75 max 75 sum 1090350
Response Body/Total Sizes : count 14538 avg 75 +/- 0 min 75 max 75 sum 1090350
All done 14538 calls (plus 1 warmup) 0.137 ms avg, 7267.8 qps
running command: kubectl exec -it netperf-pod-77c9846498-cwqf4 -- fortio load -qps 0 -c 1 -grpc -ping -t 2s 10.244.0.33 
Fortio 1.3.0 running at 0 queries per second, 4->4 procs, for 2s: 10.244.0.33
18:44:19 I grpcrunner.go:152> Starting GRPC Ping test for 10.244.0.33 with 1*1 threads at -1.0 qps
Starting at max qps with 1 thread(s) [gomax 4] for 2s
18:44:21 I periodic.go:533> T000 ended after 2.001285274s : 6432 calls. qps=3213.9346067061506
Ended after 2.001353538s : 6432 calls. qps=3213.8
Aggregated Function Time : count 6432 avg 0.00031056919 +/- 0.0005783 min 8.6875e-05 max 0.021696074 sum 1.99758104
# range, mid point, percentile, count
>= 8.6875e-05 <= 0.001 , 0.000543437 , 98.10, 6310
> 0.001 <= 0.002 , 0.0015 , 99.27, 75
> 0.002 <= 0.003 , 0.0025 , 99.55, 18
> 0.003 <= 0.004 , 0.0035 , 99.70, 10
> 0.004 <= 0.005 , 0.0045 , 99.75, 3
> 0.005 <= 0.006 , 0.0055 , 99.80, 3
> 0.006 <= 0.007 , 0.0065 , 99.84, 3
> 0.007 <= 0.008 , 0.0075 , 99.88, 2
> 0.008 <= 0.009 , 0.0085 , 99.91, 2
> 0.009 <= 0.01 , 0.0095 , 99.92, 1
> 0.011 <= 0.012 , 0.0115 , 99.94, 1
> 0.012 <= 0.014 , 0.013 , 99.95, 1
> 0.014 <= 0.016 , 0.015 , 99.98, 2
> 0.02 <= 0.0216961 , 0.020848 , 100.00, 1
# target 50% 0.000552194
# target 75% 0.000784926
# target 90% 0.000924565
# target 99% 0.00176907
# target 99.9% 0.008784
Ping SERVING : 6432
All done 6432 calls (plus 1 warmup) 0.311 ms avg, 3213.8 qps
running command: kubectl exec -it netperf-pod-77c9846498-cwqf4 -- iperf -c 10.244.1.119 -i 1 -t 2 
------------------------------------------------------------
Client connecting to 10.244.1.119, TCP port 5001
TCP window size: 85.0 KByte (default)
------------------------------------------------------------
[  3] local 10.244.0.56 port 37550 connected with 10.244.1.119 port 5001
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0- 1.0 sec   251 MBytes  2.11 Gbits/sec
[  3]  1.0- 2.0 sec   233 MBytes  1.95 Gbits/sec
[  3]  0.0- 2.0 sec   484 MBytes  2.02 Gbits/sec
running command: kubectl exec -it netperf-pod-77c9846498-cwqf4 -- netperf -H 10.244.1.119 -l 2 -P 1 -t TCP_RR -- -r 32,1024 -o P50_LATENCY,P90_LATENCY,P99_LATENCY,THROUGHPUT,THROUGHPUT_UNITS 
MIGRATED TCP REQUEST/RESPONSE TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 10.244.1.119 (10.244.) port 0 AF_INET : first burst 0
50th Percentile Latency Microseconds,90th Percentile Latency Microseconds,99th Percentile Latency Microseconds,Throughput,Throughput Units
674,1254,6500,1058.71,Trans/s
running command: kubectl exec -it netperf-pod-77c9846498-cwqf4 -- netperf -H 10.244.1.119 -l 2 -P 1 -t TCP_CRR -- -r 32,1024 -o THROUGHPUT,THROUGHPUT_UNITS 
MIGRATED TCP Connect/Request/Response TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 10.244.1.119 (10.244.) port 0 AF_INET
Throughput,Throughput Units
300.98,Trans/s
running command: kubectl exec -it netperf-pod-77c9846498-cwqf4 -- fortio load -qps 0 -c 1 -http1.0 -t 2s 10.244.1.119:8080 
Fortio 1.3.0 running at 0 queries per second, 4->4 procs, for 2s: 10.244.1.119:8080
18:44:35 I httprunner.go:82> Starting http test for 10.244.1.119:8080 with 1 threads at -1.0 qps
18:44:35 W http_client.go:142> Assuming http:// on missing scheme for '10.244.1.119:8080'
Starting at max qps with 1 thread(s) [gomax 4] for 2s
18:44:37 I periodic.go:533> T000 ended after 2.0011086s : 825 calls. qps=412.2714779197891
Ended after 2.001151854s : 825 calls. qps=412.26
Aggregated Function Time : count 825 avg 0.0024242753 +/- 0.001639 min 0.00137273 max 0.023627407 sum 2.00002712
# range, mid point, percentile, count
>= 0.00137273 <= 0.002 , 0.00168636 , 51.15, 422
> 0.002 <= 0.003 , 0.0025 , 86.42, 291
> 0.003 <= 0.004 , 0.0035 , 92.48, 50
> 0.004 <= 0.005 , 0.0045 , 95.52, 25
> 0.005 <= 0.006 , 0.0055 , 97.21, 14
> 0.006 <= 0.007 , 0.0065 , 97.82, 5
> 0.007 <= 0.008 , 0.0075 , 98.42, 5
> 0.008 <= 0.009 , 0.0085 , 98.55, 1
> 0.009 <= 0.01 , 0.0095 , 98.91, 3
> 0.01 <= 0.011 , 0.0105 , 99.39, 4
> 0.011 <= 0.012 , 0.0115 , 99.52, 1
> 0.014 <= 0.016 , 0.015 , 99.76, 2
> 0.016 <= 0.018 , 0.017 , 99.88, 1
> 0.02 <= 0.0236274 , 0.0218137 , 100.00, 1
# target 50% 0.00198585
# target 75% 0.00267612
# target 90% 0.00359
# target 99% 0.0101875
# target 99.9% 0.0206348
Sockets used: 826 (for perfect keepalive, would be 1)
Code 200 : 825 (100.0 %)
Response Header Sizes : count 825 avg 0 +/- 0 min 0 max 0 sum 0
Response Body/Total Sizes : count 825 avg 75 +/- 0 min 75 max 75 sum 61875
All done 825 calls (plus 1 warmup) 2.424 ms avg, 412.3 qps
running command: kubectl exec -it netperf-pod-77c9846498-cwqf4 -- fortio load -qps 0 -c 1 -t 2s 10.244.1.119:8080 
Fortio 1.3.0 running at 0 queries per second, 4->4 procs, for 2s: 10.244.1.119:8080
18:44:38 I httprunner.go:82> Starting http test for 10.244.1.119:8080 with 1 threads at -1.0 qps
18:44:38 W http_client.go:142> Assuming http:// on missing scheme for '10.244.1.119:8080'
Starting at max qps with 1 thread(s) [gomax 4] for 2s
18:44:40 I periodic.go:533> T000 ended after 2.000602103s : 1826 calls. qps=912.7252227026177
Ended after 2.000672699s : 1826 calls. qps=912.69
Aggregated Function Time : count 1826 avg 0.0010948464 +/- 0.00103 min 0.000545021 max 0.01711401 sum 1.9991895
# range, mid point, percentile, count
>= 0.000545021 <= 0.001 , 0.000772511 , 75.19, 1373
> 0.001 <= 0.002 , 0.0015 , 92.66, 319
> 0.002 <= 0.003 , 0.0025 , 96.06, 62
> 0.003 <= 0.004 , 0.0035 , 97.86, 33
> 0.004 <= 0.005 , 0.0045 , 99.01, 21
> 0.005 <= 0.006 , 0.0055 , 99.18, 3
> 0.006 <= 0.007 , 0.0065 , 99.40, 4
> 0.007 <= 0.008 , 0.0075 , 99.51, 2
> 0.008 <= 0.009 , 0.0085 , 99.67, 3
> 0.009 <= 0.01 , 0.0095 , 99.78, 2
> 0.01 <= 0.011 , 0.0105 , 99.84, 1
> 0.012 <= 0.014 , 0.013 , 99.89, 1
> 0.014 <= 0.016 , 0.015 , 99.95, 1
> 0.016 <= 0.017114 , 0.016557 , 100.00, 1
# target 50% 0.000847456
# target 75% 0.000998839
# target 90% 0.00184765
# target 99% 0.00498762
# target 99.9% 0.014348
Sockets used: 1 (for perfect keepalive, would be 1)
Code 200 : 1826 (100.0 %)
Response Header Sizes : count 1826 avg 75 +/- 0 min 75 max 75 sum 136950
Response Body/Total Sizes : count 1826 avg 75 +/- 0 min 75 max 75 sum 136950
All done 1826 calls (plus 1 warmup) 1.095 ms avg, 912.7 qps
running command: kubectl exec -it netperf-pod-77c9846498-cwqf4 -- fortio load -qps 0 -c 1 -grpc -ping -t 2s 10.244.1.119 
Fortio 1.3.0 running at 0 queries per second, 4->4 procs, for 2s: 10.244.1.119
18:44:41 I grpcrunner.go:152> Starting GRPC Ping test for 10.244.1.119 with 1*1 threads at -1.0 qps
Starting at max qps with 1 thread(s) [gomax 4] for 2s
18:44:43 I periodic.go:533> T000 ended after 2.001239488s : 1317 calls. qps=658.0921513377614
Ended after 2.0013888s : 1317 calls. qps=658.04
Aggregated Function Time : count 1317 avg 0.0015176588 +/- 0.003542 min 0.000555589 max 0.112567644 sum 1.99875659
# range, mid point, percentile, count
>= 0.000555589 <= 0.001 , 0.000777795 , 50.57, 666
> 0.001 <= 0.002 , 0.0015 , 88.84, 504
> 0.002 <= 0.003 , 0.0025 , 94.61, 76
> 0.003 <= 0.004 , 0.0035 , 96.20, 21
> 0.004 <= 0.005 , 0.0045 , 97.11, 12
> 0.005 <= 0.006 , 0.0055 , 97.95, 11
> 0.006 <= 0.007 , 0.0065 , 98.18, 3
> 0.007 <= 0.008 , 0.0075 , 98.56, 5
> 0.008 <= 0.009 , 0.0085 , 98.79, 3
> 0.009 <= 0.01 , 0.0095 , 98.94, 2
> 0.01 <= 0.011 , 0.0105 , 99.09, 2
> 0.011 <= 0.012 , 0.0115 , 99.24, 2
> 0.012 <= 0.014 , 0.013 , 99.39, 2
> 0.014 <= 0.016 , 0.015 , 99.62, 3
> 0.016 <= 0.018 , 0.017 , 99.77, 2
> 0.025 <= 0.03 , 0.0275 , 99.92, 2
> 0.1 <= 0.112568 , 0.106284 , 100.00, 1
# target 50% 0.000994988
# target 75% 0.00163839
# target 90% 0.00220132
# target 99% 0.010415
# target 99.9% 0.0292075
Ping SERVING : 1317
All done 1317 calls (plus 1 warmup) 1.518 ms avg, 658.0 qps
running command: kubectl exec -it netperf-pod-cxjzt -- iperf -c 10.244.1.119 -i 1 -t 2 
------------------------------------------------------------
Client connecting to 10.244.1.119, TCP port 5001
TCP window size: 85.0 KByte (default)
------------------------------------------------------------
[  3] local 10.244.0.160 port 35056 connected with 10.244.1.119 port 5001
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0- 1.0 sec   258 MBytes  2.16 Gbits/sec
[  3]  1.0- 2.0 sec   239 MBytes  2.00 Gbits/sec
[  3]  0.0- 2.0 sec   496 MBytes  2.08 Gbits/sec
running command: kubectl exec -it netperf-pod-cxjzt -- netperf -H 10.244.1.119 -l 2 -P 1 -t TCP_RR -- -r 32,1024 -o P50_LATENCY,P90_LATENCY,P99_LATENCY,THROUGHPUT,THROUGHPUT_UNITS 
MIGRATED TCP REQUEST/RESPONSE TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 10.244.1.119 (10.244.) port 0 AF_INET : first burst 0
50th Percentile Latency Microseconds,90th Percentile Latency Microseconds,99th Percentile Latency Microseconds,Throughput,Throughput Units
726,898,2650,1255.60,Trans/s
running command: kubectl exec -it netperf-pod-cxjzt -- netperf -H 10.244.1.119 -l 2 -P 1 -t TCP_CRR -- -r 32,1024 -o THROUGHPUT,THROUGHPUT_UNITS 
MIGRATED TCP Connect/Request/Response TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 10.244.1.119 (10.244.) port 0 AF_INET
Throughput,Throughput Units
434.49,Trans/s
running command: kubectl exec -it netperf-pod-cxjzt -- fortio load -qps 0 -c 1 -http1.0 -t 2s 10.244.1.119:8080 
Fortio 1.3.0 running at 0 queries per second, 4->4 procs, for 2s: 10.244.1.119:8080
18:44:58 I httprunner.go:82> Starting http test for 10.244.1.119:8080 with 1 threads at -1.0 qps
18:44:58 W http_client.go:142> Assuming http:// on missing scheme for '10.244.1.119:8080'
Starting at max qps with 1 thread(s) [gomax 4] for 2s
18:45:00 I periodic.go:533> T000 ended after 2.001338537s : 1033 calls. qps=516.1545540158656
Ended after 2.001398213s : 1033 calls. qps=516.14
Aggregated Function Time : count 1033 avg 0.0019366725 +/- 0.0009239 min 0.001380153 max 0.018393752 sum 2.00058264
# range, mid point, percentile, count
>= 0.00138015 <= 0.002 , 0.00169008 , 80.45, 831
> 0.002 <= 0.003 , 0.0025 , 97.68, 178
> 0.003 <= 0.004 , 0.0035 , 98.55, 9
> 0.004 <= 0.005 , 0.0045 , 99.13, 6
> 0.005 <= 0.006 , 0.0055 , 99.52, 4
> 0.006 <= 0.007 , 0.0065 , 99.61, 1
> 0.01 <= 0.011 , 0.0105 , 99.71, 1
> 0.014 <= 0.016 , 0.015 , 99.90, 2
> 0.018 <= 0.0183938 , 0.0181969 , 100.00, 1
# target 50% 0.00176513
# target 75% 0.00195799
# target 90% 0.00255449
# target 99% 0.00477833
# target 99.9% 0.015967
Sockets used: 1034 (for perfect keepalive, would be 1)
Code 200 : 1033 (100.0 %)
Response Header Sizes : count 1033 avg 0 +/- 0 min 0 max 0 sum 0
Response Body/Total Sizes : count 1033 avg 75 +/- 0 min 75 max 75 sum 77475
All done 1033 calls (plus 1 warmup) 1.937 ms avg, 516.1 qps
running command: kubectl exec -it netperf-pod-cxjzt -- fortio load -qps 0 -c 1 -t 2s 10.244.1.119:8080 
Fortio 1.3.0 running at 0 queries per second, 4->4 procs, for 2s: 10.244.1.119:8080
18:45:00 I httprunner.go:82> Starting http test for 10.244.1.119:8080 with 1 threads at -1.0 qps
18:45:00 W http_client.go:142> Assuming http:// on missing scheme for '10.244.1.119:8080'
Starting at max qps with 1 thread(s) [gomax 4] for 2s
18:45:02 I periodic.go:533> T000 ended after 2.00065214s : 2120 calls. qps=1059.6544784642072
Ended after 2.000733603s : 2120 calls. qps=1059.6
Aggregated Function Time : count 2120 avg 0.00094298667 +/- 0.0007794 min 0.000561808 max 0.020239416 sum 1.99913174
# range, mid point, percentile, count
>= 0.000561808 <= 0.001 , 0.000780904 , 83.49, 1770
> 0.001 <= 0.002 , 0.0015 , 98.35, 315
> 0.002 <= 0.003 , 0.0025 , 99.06, 15
> 0.003 <= 0.004 , 0.0035 , 99.20, 3
> 0.004 <= 0.005 , 0.0045 , 99.39, 4
> 0.005 <= 0.006 , 0.0055 , 99.53, 3
> 0.006 <= 0.007 , 0.0065 , 99.58, 1
> 0.007 <= 0.008 , 0.0075 , 99.62, 1
> 0.008 <= 0.009 , 0.0085 , 99.72, 2
> 0.009 <= 0.01 , 0.0095 , 99.81, 2
> 0.01 <= 0.011 , 0.0105 , 99.86, 1
> 0.011 <= 0.012 , 0.0115 , 99.95, 2
> 0.02 <= 0.0202394 , 0.0201197 , 100.00, 1
# target 50% 0.000824129
# target 75% 0.000955413
# target 90% 0.0014381
# target 99% 0.00292
# target 99.9% 0.01144
Sockets used: 1 (for perfect keepalive, would be 1)
Code 200 : 2120 (100.0 %)
Response Header Sizes : count 2120 avg 75 +/- 0 min 75 max 75 sum 159000
Response Body/Total Sizes : count 2120 avg 75 +/- 0 min 75 max 75 sum 159000
All done 2120 calls (plus 1 warmup) 0.943 ms avg, 1059.6 qps
running command: kubectl exec -it netperf-pod-cxjzt -- fortio load -qps 0 -c 1 -grpc -ping -t 2s 10.244.1.119 
Fortio 1.3.0 running at 0 queries per second, 4->4 procs, for 2s: 10.244.1.119
18:45:03 I grpcrunner.go:152> Starting GRPC Ping test for 10.244.1.119 with 1*1 threads at -1.0 qps
Starting at max qps with 1 thread(s) [gomax 4] for 2s
18:45:05 I periodic.go:533> T000 ended after 2.000377283s : 2049 calls. qps=1024.306773233837
Ended after 2.000445385s : 2049 calls. qps=1024.3
Aggregated Function Time : count 2049 avg 0.00097536095 +/- 0.0004217 min 0.000597246 max 0.012760913 sum 1.99851458
# range, mid point, percentile, count
>= 0.000597246 <= 0.001 , 0.000798623 , 69.84, 1431
> 0.001 <= 0.002 , 0.0015 , 98.88, 595
> 0.002 <= 0.003 , 0.0025 , 99.46, 12
> 0.003 <= 0.004 , 0.0035 , 99.61, 3
> 0.004 <= 0.005 , 0.0045 , 99.80, 4
> 0.005 <= 0.006 , 0.0055 , 99.90, 2
> 0.006 <= 0.007 , 0.0065 , 99.95, 1
> 0.012 <= 0.0127609 , 0.0123805 , 100.00, 1
# target 50% 0.000885511
# target 75% 0.00117773
# target 90% 0.00169429
# target 99% 0.00220917
# target 99.9% 0.0059755
Ping SERVING : 2049
All done 2049 calls (plus 1 warmup) 0.975 ms avg, 1024.3 qps
running command: kubectl exec -it netperf-pod-77c9846498-cwqf4 -- iperf -c 10.245.186.73 -i 1 -t 2 
------------------------------------------------------------
Client connecting to 10.245.186.73, TCP port 5001
TCP window size: 85.0 KByte (default)
------------------------------------------------------------
[  3] local 10.244.0.56 port 41888 connected with 10.245.186.73 port 5001
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0- 1.0 sec   148 MBytes  1.24 Gbits/sec
[  3]  1.0- 2.0 sec   166 MBytes  1.39 Gbits/sec
[  3]  0.0- 2.0 sec   314 MBytes  1.32 Gbits/sec
running command: kubectl exec -it netperf-pod-77c9846498-cwqf4 -- fortio load -qps 0 -c 1 -http1.0 -t 2s 10.245.186.73:8080 
Fortio 1.3.0 running at 0 queries per second, 4->4 procs, for 2s: 10.245.186.73:8080
18:45:09 I httprunner.go:82> Starting http test for 10.245.186.73:8080 with 1 threads at -1.0 qps
18:45:09 W http_client.go:142> Assuming http:// on missing scheme for '10.245.186.73:8080'
Starting at max qps with 1 thread(s) [gomax 4] for 2s
18:45:11 I periodic.go:533> T000 ended after 2.001810738s : 595 calls. qps=297.2308963605929
Ended after 2.001893581s : 595 calls. qps=297.22
Aggregated Function Time : count 595 avg 0.003356414 +/- 0.003466 min 0.000311656 max 0.046177447 sum 1.99706631
# range, mid point, percentile, count
>= 0.000311656 <= 0.001 , 0.000655828 , 25.71, 153
> 0.001 <= 0.002 , 0.0015 , 35.97, 61
> 0.002 <= 0.003 , 0.0025 , 56.97, 125
> 0.003 <= 0.004 , 0.0035 , 75.46, 110
> 0.004 <= 0.005 , 0.0045 , 83.70, 49
> 0.005 <= 0.006 , 0.0055 , 86.72, 18
> 0.006 <= 0.007 , 0.0065 , 89.92, 19
> 0.007 <= 0.008 , 0.0075 , 91.93, 12
> 0.008 <= 0.009 , 0.0085 , 93.11, 7
> 0.009 <= 0.01 , 0.0095 , 94.79, 10
> 0.01 <= 0.011 , 0.0105 , 96.13, 8
> 0.011 <= 0.012 , 0.0115 , 97.48, 8
> 0.012 <= 0.014 , 0.013 , 98.49, 6
> 0.014 <= 0.016 , 0.015 , 99.33, 5
> 0.016 <= 0.018 , 0.017 , 99.66, 2
> 0.02 <= 0.025 , 0.0225 , 99.83, 1
> 0.045 <= 0.0461774 , 0.0455887 , 100.00, 1
# target 50% 0.002668
# target 75% 0.003975
# target 90% 0.00704167
# target 99% 0.01522
# target 99.9% 0.0454769
Sockets used: 596 (for perfect keepalive, would be 1)
Code 200 : 595 (100.0 %)
Response Header Sizes : count 595 avg 0 +/- 0 min 0 max 0 sum 0
Response Body/Total Sizes : count 595 avg 75 +/- 0 min 75 max 75 sum 44625
All done 595 calls (plus 1 warmup) 3.356 ms avg, 297.2 qps
running command: kubectl exec -it netperf-pod-77c9846498-cwqf4 -- fortio load -qps 0 -c 1 -t 2s 10.245.186.73:8080 
Fortio 1.3.0 running at 0 queries per second, 4->4 procs, for 2s: 10.245.186.73:8080
18:45:12 I httprunner.go:82> Starting http test for 10.245.186.73:8080 with 1 threads at -1.0 qps
18:45:12 W http_client.go:142> Assuming http:// on missing scheme for '10.245.186.73:8080'
Starting at max qps with 1 thread(s) [gomax 4] for 2s
18:45:14 I periodic.go:533> T000 ended after 2.000188986s : 1431 calls. qps=715.4323966465437
Ended after 2.000277864s : 1431 calls. qps=715.4
Aggregated Function Time : count 1431 avg 0.0013964267 +/- 0.002416 min 0.000575811 max 0.051750731 sum 1.99828667
# range, mid point, percentile, count
>= 0.000575811 <= 0.001 , 0.000787905 , 58.91, 843
> 0.001 <= 0.002 , 0.0015 , 90.08, 446
> 0.002 <= 0.003 , 0.0025 , 94.20, 59
> 0.003 <= 0.004 , 0.0035 , 96.16, 28
> 0.004 <= 0.005 , 0.0045 , 97.27, 16
> 0.005 <= 0.006 , 0.0055 , 97.83, 8
> 0.006 <= 0.007 , 0.0065 , 98.46, 9
> 0.007 <= 0.008 , 0.0075 , 99.02, 8
> 0.008 <= 0.009 , 0.0085 , 99.30, 4
> 0.009 <= 0.01 , 0.0095 , 99.44, 2
> 0.012 <= 0.014 , 0.013 , 99.51, 1
> 0.014 <= 0.016 , 0.015 , 99.65, 2
> 0.016 <= 0.018 , 0.017 , 99.72, 1
> 0.018 <= 0.02 , 0.019 , 99.79, 1
> 0.035 <= 0.04 , 0.0375 , 99.86, 1
> 0.04 <= 0.045 , 0.0425 , 99.93, 1
> 0.05 <= 0.0517507 , 0.0508754 , 100.00, 1
# target 50% 0.000935767
# target 75% 0.00151626
# target 90% 0.00199753
# target 99% 0.00796125
# target 99.9% 0.042845
Sockets used: 1 (for perfect keepalive, would be 1)
Code 200 : 1431 (100.0 %)
Response Header Sizes : count 1431 avg 75 +/- 0 min 75 max 75 sum 107325
Response Body/Total Sizes : count 1431 avg 75 +/- 0 min 75 max 75 sum 107325
All done 1431 calls (plus 1 warmup) 1.396 ms avg, 715.4 qps
running command: kubectl exec -it netperf-pod-77c9846498-cwqf4 -- fortio load -qps 0 -c 1 -grpc -ping -t 2s 10.245.186.73 
Fortio 1.3.0 running at 0 queries per second, 4->4 procs, for 2s: 10.245.186.73
18:45:15 I grpcrunner.go:152> Starting GRPC Ping test for 10.245.186.73 with 1*1 threads at -1.0 qps
Starting at max qps with 1 thread(s) [gomax 4] for 2s
18:45:17 I periodic.go:533> T000 ended after 2.00013619s : 1371 calls. qps=685.4533240558984
Ended after 2.000236688s : 1371 calls. qps=685.42
Aggregated Function Time : count 1371 avg 0.0014575512 +/- 0.001481 min 0.00058691 max 0.022356447 sum 1.9983027
# range, mid point, percentile, count
>= 0.00058691 <= 0.001 , 0.000793455 , 43.62, 598
> 0.001 <= 0.002 , 0.0015 , 87.02, 595
> 0.002 <= 0.003 , 0.0025 , 93.36, 87
> 0.003 <= 0.004 , 0.0035 , 96.21, 39
> 0.004 <= 0.005 , 0.0045 , 97.16, 13
> 0.005 <= 0.006 , 0.0055 , 97.74, 8
> 0.006 <= 0.007 , 0.0065 , 98.40, 9
> 0.007 <= 0.008 , 0.0075 , 98.83, 6
> 0.008 <= 0.009 , 0.0085 , 99.12, 4
> 0.009 <= 0.01 , 0.0095 , 99.34, 3
> 0.01 <= 0.011 , 0.0105 , 99.49, 2
> 0.011 <= 0.012 , 0.0115 , 99.64, 2
> 0.012 <= 0.014 , 0.013 , 99.85, 3
> 0.014 <= 0.016 , 0.015 , 99.93, 1
> 0.02 <= 0.0223564 , 0.0211782 , 100.00, 1
# target 50% 0.00114706
# target 75% 0.00172311
# target 90% 0.00247011
# target 99% 0.0085725
# target 99.9% 0.015258
Ping SERVING : 1371
All done 1371 calls (plus 1 warmup) 1.458 ms avg, 685.4 qps
running command: kubectl exec -it netperf-pod-cxjzt -- iperf -c 10.245.186.73 -i 1 -t 2 
------------------------------------------------------------
Client connecting to 10.245.186.73, TCP port 5001
TCP window size: 85.0 KByte (default)
------------------------------------------------------------
[  3] local 10.244.0.160 port 33530 connected with 10.245.186.73 port 5001
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0- 1.0 sec   256 MBytes  2.15 Gbits/sec
[  3]  1.0- 2.0 sec   239 MBytes  2.01 Gbits/sec
[  3]  0.0- 2.0 sec   495 MBytes  2.07 Gbits/sec
running command: kubectl exec -it netperf-pod-cxjzt -- fortio load -qps 0 -c 1 -http1.0 -t 2s 10.245.186.73:8080 
Fortio 1.3.0 running at 0 queries per second, 4->4 procs, for 2s: 10.245.186.73:8080
18:45:21 I httprunner.go:82> Starting http test for 10.245.186.73:8080 with 1 threads at -1.0 qps
18:45:21 W http_client.go:142> Assuming http:// on missing scheme for '10.245.186.73:8080'
Starting at max qps with 1 thread(s) [gomax 4] for 2s
18:45:23 I periodic.go:533> T000 ended after 2.000198031s : 742 calls. qps=370.96326888644955
Ended after 2.000278101s : 742 calls. qps=370.95
Aggregated Function Time : count 742 avg 0.002694645 +/- 0.003041 min 0.000264842 max 0.051930643 sum 1.9994266
# range, mid point, percentile, count
>= 0.000264842 <= 0.001 , 0.000632421 , 23.05, 171
> 0.001 <= 0.002 , 0.0015 , 35.98, 96
> 0.002 <= 0.003 , 0.0025 , 78.17, 313
> 0.003 <= 0.004 , 0.0035 , 86.93, 65
> 0.004 <= 0.005 , 0.0045 , 89.62, 20
> 0.005 <= 0.006 , 0.0055 , 92.99, 25
> 0.006 <= 0.007 , 0.0065 , 94.34, 10
> 0.007 <= 0.008 , 0.0075 , 96.09, 13
> 0.008 <= 0.009 , 0.0085 , 97.17, 8
> 0.009 <= 0.01 , 0.0095 , 97.98, 6
> 0.01 <= 0.011 , 0.0105 , 98.25, 2
> 0.011 <= 0.012 , 0.0115 , 98.79, 4
> 0.012 <= 0.014 , 0.013 , 99.06, 2
> 0.016 <= 0.018 , 0.017 , 99.60, 4
> 0.02 <= 0.025 , 0.0225 , 99.87, 2
> 0.05 <= 0.0519306 , 0.0509653 , 100.00, 1
# target 50% 0.00233227
# target 75% 0.00292492
# target 90% 0.005112
# target 99% 0.01358
# target 99.9% 0.0504981
Sockets used: 743 (for perfect keepalive, would be 1)
Code 200 : 742 (100.0 %)
Response Header Sizes : count 742 avg 0 +/- 0 min 0 max 0 sum 0
Response Body/Total Sizes : count 742 avg 75 +/- 0 min 75 max 75 sum 55650
All done 742 calls (plus 1 warmup) 2.695 ms avg, 370.9 qps
running command: kubectl exec -it netperf-pod-cxjzt -- fortio load -qps 0 -c 1 -t 2s 10.245.186.73:8080 
Fortio 1.3.0 running at 0 queries per second, 4->4 procs, for 2s: 10.245.186.73:8080
18:45:24 I httprunner.go:82> Starting http test for 10.245.186.73:8080 with 1 threads at -1.0 qps
18:45:24 W http_client.go:142> Assuming http:// on missing scheme for '10.245.186.73:8080'
Starting at max qps with 1 thread(s) [gomax 4] for 2s
18:45:26 I periodic.go:533> T000 ended after 2.000422965s : 1471 calls. qps=735.3444875094203
Ended after 2.000495149s : 1471 calls. qps=735.32
Aggregated Function Time : count 1471 avg 0.0013592199 +/- 0.001639 min 0.000590022 max 0.020528066 sum 1.99941249
# range, mid point, percentile, count
>= 0.000590022 <= 0.001 , 0.000795011 , 62.61, 921
> 0.001 <= 0.002 , 0.0015 , 90.28, 407
> 0.002 <= 0.003 , 0.0025 , 93.47, 47
> 0.003 <= 0.004 , 0.0035 , 95.24, 26
> 0.004 <= 0.005 , 0.0045 , 96.46, 18
> 0.005 <= 0.006 , 0.0055 , 97.21, 11
> 0.006 <= 0.007 , 0.0065 , 97.89, 10
> 0.007 <= 0.008 , 0.0075 , 98.64, 11
> 0.008 <= 0.009 , 0.0085 , 98.91, 4
> 0.009 <= 0.01 , 0.0095 , 99.25, 5
> 0.01 <= 0.011 , 0.0105 , 99.46, 3
> 0.011 <= 0.012 , 0.0115 , 99.59, 2
> 0.012 <= 0.014 , 0.013 , 99.73, 2
> 0.014 <= 0.016 , 0.015 , 99.80, 1
> 0.018 <= 0.02 , 0.019 , 99.93, 2
> 0.02 <= 0.0205281 , 0.020264 , 100.00, 1
# target 50% 0.000917336
# target 75% 0.00144779
# target 90% 0.00198993
# target 99% 0.009258
# target 99.9% 0.019529
Sockets used: 1 (for perfect keepalive, would be 1)
Code 200 : 1471 (100.0 %)
Response Header Sizes : count 1471 avg 75 +/- 0 min 75 max 75 sum 110325
Response Body/Total Sizes : count 1471 avg 75 +/- 0 min 75 max 75 sum 110325
All done 1471 calls (plus 1 warmup) 1.359 ms avg, 735.3 qps
running command: kubectl exec -it netperf-pod-cxjzt -- fortio load -qps 0 -c 1 -grpc -ping -t 2s 10.245.186.73 
Fortio 1.3.0 running at 0 queries per second, 4->4 procs, for 2s: 10.245.186.73
18:45:26 I grpcrunner.go:152> Starting GRPC Ping test for 10.245.186.73 with 1*1 threads at -1.0 qps
Starting at max qps with 1 thread(s) [gomax 4] for 2s
18:45:28 I periodic.go:533> T000 ended after 2.000118577s : 11463 calls. qps=5731.160208108002
Ended after 2.000181798s : 11463 calls. qps=5731
Aggregated Function Time : count 11463 avg 0.0001740661 +/- 0.0001437 min 7.6769e-05 max 0.0060698 sum 1.99531974
# range, mid point, percentile, count
>= 7.6769e-05 <= 0.001 , 0.000538384 , 99.55, 11411
> 0.001 <= 0.002 , 0.0015 , 99.90, 40
> 0.002 <= 0.003 , 0.0025 , 99.94, 5
> 0.003 <= 0.004 , 0.0035 , 99.99, 6
> 0.006 <= 0.0060698 , 0.0060349 , 100.00, 1
# target 50% 0.000540448
# target 75% 0.000772328
# target 90% 0.000911456
# target 99% 0.000994932
# target 99.9% 0.0021074
Ping SERVING : 11463
All done 11463 calls (plus 1 warmup) 0.174 ms avg, 5731.0 qps
